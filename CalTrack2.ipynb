{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f6a4e6-5b09-42ac-9e1f-a36e695941b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mediapipe in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: absl-py in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (2.3.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (0.6.1)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (0.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from jax->mediapipe) (1.15.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d59852-2f77-46cb-a233-3d8142b70c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b0679d-dcd4-426b-998b-2a7f4e221bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Mediapipe Feed',frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1548116e-b2f4-4ae6-8d80-1d91a309f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set up Mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        #render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66),thickness=2,circle_radius=2),\n",
    "                                  mp_drawing.DrawingSpec(color=(245,66,230),thickness=2,circle_radius=2)\n",
    "                                 )\n",
    "        \n",
    "        # Print results (you can access landmarks via results.pose_landmarks)\n",
    "        print(results)\n",
    "\n",
    "        # Display the image (can also draw landmarks if needed)\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a16c45-df08-442d-beeb-d35362987e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pose_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac7aa2-0ed8-4282-baf8-73e6f7535ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose.POSE_CONNECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898eb786-b90f-49df-8ecf-366f4dd9a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing.draw_landmarks??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09bfbf1-f5f6-4771-b80c-9214420d0ce6",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/3j8BPdc.png\" style=\"height:300px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c114f144-93ee-437b-887c-4dd727fca7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set up Mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        #extract landmarks\n",
    "        try:\n",
    "            landmarks=results.pose_landmarks.landmark\n",
    "            print(landmarks)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66),thickness=2,circle_radius=2),\n",
    "                                  mp_drawing.DrawingSpec(color=(245,66,230),thickness=2,circle_radius=2)\n",
    "                                 )\n",
    "        \n",
    "        # Print results (you can access landmarks via results.pose_landmarks)\n",
    "        print(results)\n",
    "\n",
    "        # Display the image (can also draw landmarks if needed)\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8316ec6-0ba8-4855-9371-04fd01adc42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results.pose_landmarks:\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "    print(\"Number of landmarks:\", len(landmarks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8310877-f3b9-4067-ac00-6facff25c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ldmrk in mp_pose.PoseLandmark:\n",
    "    print(ldmrk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c6a76-55db-49b1-bdf7-80629a2ddd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2280b54-d166-4cf9-8d85-c07782ddeda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8eb18-28ea-4c50-8f87-1a11e5f8c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9dee46-18e1-46e6-a773-bfa9b34e4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a=np.array(a)#first\n",
    "    b=np.array(b)#mid\n",
    "    c=np.array(c)#end\n",
    "\n",
    "    radians=np.arctan2(c[1]-b[1],c[0]-b[0])-np.arctan2(a[1]-b[1],a[0]-b[0])\n",
    "    angle=np.abs(radians*180.0/np.pi)\n",
    "\n",
    "    if angle>180.0:\n",
    "        angle=360-angle\n",
    "\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c26eb-31a4-474a-bbe3-dffa6f2b1471",
   "metadata": {},
   "outputs": [],
   "source": [
    "shoulder=[landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "elbow=[landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "wrist=[landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f38b5-4297-4838-8638-19a0cfe112ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "shoulder,elbow,wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8133d-d307-4625-bec3-e0424a5c9a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_angle(shoulder,elbow,wrist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f008157-87a0-40ec-8b25-0a5fdaf4363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(elbow,[640,480]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b879dae-f844-40ba-b514-a0ca6a0cdab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curl count: 1\n",
      "Curl count: 2\n",
      "Curl count: 3\n",
      "Curl count: 4\n",
      "Curl count: 5\n",
      "Curl count: 6\n",
      "Curl count: 7\n",
      "Curl count: 8\n",
      "Curl count: 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 123\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;66;03m# Display the image\u001b[39;00m\n\u001b[0;32m    121\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBicep Curl Counter\u001b[39m\u001b[38;5;124m'\u001b[39m, image)\n\u001b[1;32m--> 123\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Function to calculate angle between three points\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calculate angle between three points\n",
    "    a, b, c are lists/arrays of [x, y] coordinates\n",
    "    b is the vertex point (elbow in this case)\n",
    "    \"\"\"\n",
    "    a = np.array(a)  # First point (shoulder)\n",
    "    b = np.array(b)  # Vertex point (elbow)\n",
    "    c = np.array(c)  # End point (wrist)\n",
    "    \n",
    "    # Calculate vectors\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    \n",
    "    # Calculate angle using dot product\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    \n",
    "    # Convert to degrees\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Curl counter variables\n",
    "counter = 0\n",
    "stage = None\n",
    "\n",
    "# Set up Mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "            \n",
    "        # Get frame dimensions\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                \n",
    "                # Get coordinates (normalized)\n",
    "                shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "                elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "                wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "                \n",
    "                # Calculate angle\n",
    "                angle = calculate_angle(shoulder, elbow, wrist)\n",
    "                \n",
    "                # Convert elbow coordinates to pixel coordinates\n",
    "                elbow_coords = tuple(np.multiply(elbow, [frame_width, frame_height]).astype(int))\n",
    "                \n",
    "                # Draw circle and put angle text (using 'deg' instead of degree symbol)\n",
    "                cv2.circle(image, elbow_coords, 10, (0, 255, 0), -1)\n",
    "                cv2.putText(image, f'{int(angle)} deg',\n",
    "                           (elbow_coords[0] - 40, elbow_coords[1] - 20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                # Curl counter logic (properly indented within the try block)\n",
    "                if angle > 160:\n",
    "                    stage = 'down'\n",
    "                if angle < 30 and stage == 'down':\n",
    "                    stage = \"up\"\n",
    "                    counter += 1\n",
    "                    print(f\"Curl count: {counter}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "        \n",
    "        # Setup status box for displaying counter and stage\n",
    "        cv2.rectangle(image, (0, 0), (300, 100), (245, 117, 16), -1)\n",
    "        \n",
    "        # Rep data\n",
    "        cv2.putText(image, 'REPS', (15, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(counter), (15, 70), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Stage data\n",
    "        cv2.putText(image, 'STAGE', (150, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(stage), (150, 70), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Render detections\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "        \n",
    "        # Display the image\n",
    "        cv2.imshow('Bicep Curl Counter', image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99f80e-0aa9-4217-8bc8-e3f88d1c27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Function to calculate angle between three points\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calculate angle between three points\n",
    "    a, b, c are lists/arrays of [x, y] coordinates\n",
    "    b is the vertex point (knee in this case)\n",
    "    \"\"\"\n",
    "    a = np.array(a)  # First point (hip)\n",
    "    b = np.array(b)  # Vertex point (knee)\n",
    "    c = np.array(c)  # End point (ankle)\n",
    "    \n",
    "    # Calculate vectors\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    \n",
    "    # Calculate angle using dot product\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    \n",
    "    # Convert to degrees\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Squat counter variables\n",
    "counter = 0\n",
    "stage = None\n",
    "\n",
    "# Set up Mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "            \n",
    "        # Get frame dimensions\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                \n",
    "                # Get coordinates for left leg (normalized)\n",
    "                left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "                left_knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "                left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "                \n",
    "                # Get coordinates for right leg (normalized)\n",
    "                right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "                right_knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "                right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                              landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "                \n",
    "                # Calculate angles for both knees\n",
    "                left_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "                right_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "                \n",
    "                # Use average of both knee angles for squat detection\n",
    "                avg_angle = (left_angle + right_angle) / 2\n",
    "                \n",
    "                # Convert knee coordinates to pixel coordinates for display\n",
    "                left_knee_coords = tuple(np.multiply(left_knee, [frame_width, frame_height]).astype(int))\n",
    "                right_knee_coords = tuple(np.multiply(right_knee, [frame_width, frame_height]).astype(int))\n",
    "                \n",
    "                # Draw circles and put angle text at both knees\n",
    "                cv2.circle(image, left_knee_coords, 8, (0, 255, 0), -1)\n",
    "                cv2.putText(image, f'{int(left_angle)} deg',\n",
    "                           (left_knee_coords[0] - 50, left_knee_coords[1] - 20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                cv2.circle(image, right_knee_coords, 8, (0, 255, 0), -1)\n",
    "                cv2.putText(image, f'{int(right_angle)} deg',\n",
    "                           (right_knee_coords[0] + 10, right_knee_coords[1] - 20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                # Squat counter logic\n",
    "                # When standing up (knees extended)\n",
    "                if avg_angle > 160:\n",
    "                    stage = 'up'\n",
    "                \n",
    "                # When squatting down (knees bent) and previous stage was up\n",
    "                if avg_angle < 90 and stage == 'up':\n",
    "                    stage = 'down'\n",
    "                    counter += 1\n",
    "                    print(f\"Squat count: {counter}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "        \n",
    "        # Setup status box for displaying counter and stage\n",
    "        cv2.rectangle(image, (0, 0), (350, 120), (245, 117, 16), -1)\n",
    "        \n",
    "        # Rep data\n",
    "        cv2.putText(image, 'SQUATS', (15, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(counter), (15, 80), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Stage data\n",
    "        cv2.putText(image, 'STAGE', (150, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(stage), (150, 80), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Average angle data\n",
    "        try:\n",
    "            cv2.putText(image, 'AVG ANGLE', (240, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, f'{int(avg_angle)} deg', (240, 80), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Render pose detections\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "        \n",
    "        # Add instructions text\n",
    "        cv2.putText(image, 'Stand straight to start, squat down to count reps', \n",
    "                    (10, frame_height - 20), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Display the image\n",
    "        cv2.imshow('Squat Counter', image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dffb715b-b613-4837-84e4-f2c4ea22964c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left lunge completed! Count: 1\n",
      "Right lunge completed! Count: 1\n",
      "Right lunge completed! Count: 2\n",
      "Right lunge completed! Count: 3\n",
      "Right lunge completed! Count: 4\n",
      "Right lunge completed! Count: 5\n",
      "Left lunge completed! Count: 2\n",
      "Right lunge completed! Count: 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Make detection\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m results \u001b[38;5;241m=\u001b[39m pose\u001b[38;5;241m.\u001b[39mprocess(image)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Recolor back to BGR\u001b[39;00m\n\u001b[0;32m     90\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mprocess(input_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image})\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mwait_until_idle()\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Function to calculate angle between three points\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calculate angle between three points\n",
    "    a, b, c are lists/arrays of [x, y] coordinates\n",
    "    b is the vertex point (knee in this case)\n",
    "    \"\"\"\n",
    "    a = np.array(a)  # First point (hip)\n",
    "    b = np.array(b)  # Vertex point (knee)\n",
    "    c = np.array(c)  # End point (ankle)\n",
    "    \n",
    "    # Calculate vectors\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    \n",
    "    # Calculate angle using dot product\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    \n",
    "    # Convert to degrees\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Function to detect which leg is forward (lunging)\n",
    "def detect_forward_leg(left_knee, right_knee, left_ankle, right_ankle):\n",
    "    \"\"\"\n",
    "    Detect which leg is forward based on knee and ankle positions\n",
    "    Returns 'left', 'right', or 'none'\n",
    "    \"\"\"\n",
    "    # Calculate forward displacement (negative y means higher on screen/forward)\n",
    "    left_forward_score = left_knee[1] - left_ankle[1]  # knee higher than ankle indicates forward lunge\n",
    "    right_forward_score = right_knee[1] - right_ankle[1]\n",
    "    \n",
    "    # Also consider knee positions relative to each other\n",
    "    knee_diff = abs(left_knee[1] - right_knee[1])\n",
    "    \n",
    "    # Threshold for detecting a lunge position\n",
    "    forward_threshold = 0.05  # Adjust based on testing\n",
    "    \n",
    "    if knee_diff > forward_threshold:\n",
    "        if left_knee[1] < right_knee[1]:  # Left knee is higher (forward)\n",
    "            return 'left'\n",
    "        else:  # Right knee is higher (forward)\n",
    "            return 'right'\n",
    "    \n",
    "    return 'none'  # Standing position or unclear\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Lunge counter variables\n",
    "left_counter = 0\n",
    "right_counter = 0\n",
    "total_counter = 0\n",
    "left_stage = None\n",
    "right_stage = None\n",
    "current_leg = 'none'\n",
    "\n",
    "# Lunge detection thresholds\n",
    "LUNGE_DOWN_THRESHOLD = 90   # Lunge down position\n",
    "LUNGE_UP_THRESHOLD = 160    # Standing position\n",
    "\n",
    "# Set up Mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "            \n",
    "        # Get frame dimensions\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Initialize variables\n",
    "        left_knee_angle = 0\n",
    "        right_knee_angle = 0\n",
    "        active_angle = 0\n",
    "        active_leg = 'none'\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                \n",
    "                # Get coordinates for both legs (normalized)\n",
    "                left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "                right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "                \n",
    "                left_knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "                right_knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "                \n",
    "                left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "                right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                              landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "                \n",
    "                # Calculate knee angles for both legs\n",
    "                left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "                right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "                \n",
    "                # Detect which leg is forward (lunging)\n",
    "                current_leg = detect_forward_leg(left_knee, right_knee, left_ankle, right_ankle)\n",
    "                \n",
    "                # Determine active leg and angle based on detection\n",
    "                if current_leg == 'left':\n",
    "                    active_leg = 'Left'\n",
    "                    active_angle = left_knee_angle\n",
    "                    active_stage = left_stage\n",
    "                elif current_leg == 'right':\n",
    "                    active_leg = 'Right'\n",
    "                    active_angle = right_knee_angle\n",
    "                    active_stage = right_stage\n",
    "                else:\n",
    "                    active_leg = 'Standing'\n",
    "                    active_angle = min(left_knee_angle, right_knee_angle)  # Show the more bent knee\n",
    "                    active_stage = None\n",
    "                \n",
    "                # Lunge counting logic for left leg\n",
    "                if current_leg == 'left':\n",
    "                    if left_knee_angle > LUNGE_UP_THRESHOLD:\n",
    "                        left_stage = 'up'\n",
    "                    if left_knee_angle < LUNGE_DOWN_THRESHOLD and left_stage == 'up':\n",
    "                        left_stage = 'down'\n",
    "                        left_counter += 1\n",
    "                        total_counter += 1\n",
    "                        print(f\"Left lunge completed! Count: {left_counter}\")\n",
    "                \n",
    "                # Lunge counting logic for right leg\n",
    "                elif current_leg == 'right':\n",
    "                    if right_knee_angle > LUNGE_UP_THRESHOLD:\n",
    "                        right_stage = 'up'\n",
    "                    if right_knee_angle < LUNGE_DOWN_THRESHOLD and right_stage == 'up':\n",
    "                        right_stage = 'down'\n",
    "                        right_counter += 1\n",
    "                        total_counter += 1\n",
    "                        print(f\"Right lunge completed! Count: {right_counter}\")\n",
    "                \n",
    "                # Convert knee coordinates to pixel coordinates for visualization\n",
    "                left_knee_coords = tuple(np.multiply(left_knee, [frame_width, frame_height]).astype(int))\n",
    "                right_knee_coords = tuple(np.multiply(right_knee, [frame_width, frame_height]).astype(int))\n",
    "                \n",
    "                # Draw angle indicators on both knees with different colors based on activity\n",
    "                if current_leg == 'left':\n",
    "                    # Highlight left knee as active\n",
    "                    cv2.circle(image, left_knee_coords, 12, (0, 255, 0), -1)  # Green for active\n",
    "                    cv2.circle(image, right_knee_coords, 8, (100, 100, 100), -1)  # Gray for inactive\n",
    "                elif current_leg == 'right':\n",
    "                    # Highlight right knee as active\n",
    "                    cv2.circle(image, left_knee_coords, 8, (100, 100, 100), -1)  # Gray for inactive\n",
    "                    cv2.circle(image, right_knee_coords, 12, (0, 255, 0), -1)  # Green for active\n",
    "                else:\n",
    "                    # Both knees neutral\n",
    "                    cv2.circle(image, left_knee_coords, 8, (0, 255, 255), -1)  # Yellow for neutral\n",
    "                    cv2.circle(image, right_knee_coords, 8, (0, 255, 255), -1)  # Yellow for neutral\n",
    "                \n",
    "                # Display angle text for both knees\n",
    "                cv2.putText(image, f'{int(left_knee_angle)}°',\n",
    "                           (left_knee_coords[0] - 50, left_knee_coords[1] - 20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                cv2.putText(image, f'{int(right_knee_angle)}°',\n",
    "                           (right_knee_coords[0] + 10, right_knee_coords[1] - 20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "        \n",
    "        # Setup main status display\n",
    "        cv2.rectangle(image, (10, 10), (450, 200), (50, 50, 50), -1)\n",
    "        cv2.rectangle(image, (10, 10), (450, 200), (255, 255, 255), 2)\n",
    "        \n",
    "        # Display total lunges\n",
    "        cv2.putText(image, 'TOTAL LUNGES', (20, 40), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(total_counter), (20, 75), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display individual leg counts\n",
    "        cv2.putText(image, f'Left: {left_counter}', (180, 50), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'Right: {right_counter}', (180, 75), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display active leg and stage\n",
    "        cv2.putText(image, 'ACTIVE LEG', (300, 40), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, active_leg, (300, 70), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display current stage\n",
    "        current_stage = 'None'\n",
    "        if current_leg == 'left' and left_stage:\n",
    "            current_stage = left_stage.capitalize()\n",
    "        elif current_leg == 'right' and right_stage:\n",
    "            current_stage = right_stage.capitalize()\n",
    "        elif current_leg == 'none':\n",
    "            current_stage = 'Standing'\n",
    "            \n",
    "        cv2.putText(image, 'STAGE', (20, 110), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, current_stage, (20, 140), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display active knee angle\n",
    "        cv2.putText(image, 'KNEE ANGLE', (200, 110), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'{int(active_angle)}°', (200, 140), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display both individual knee angles\n",
    "        cv2.putText(image, f'L Knee: {int(left_knee_angle)}°', (20, 170), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'R Knee: {int(right_knee_angle)}°', (180, 170), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Show lunge detection status\n",
    "        detection_color = (0, 255, 0) if current_leg != 'none' else (100, 100, 100)\n",
    "        cv2.putText(image, f'Detection: {current_leg.upper()}', (300, 170), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, detection_color, 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Render pose detections\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "        \n",
    "        # Add instructions\n",
    "        cv2.putText(image, 'Perform alternating lunges. Step forward and bend knee. Press Q to quit.', \n",
    "                    (10, frame_height - 20), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Display the image\n",
    "        cv2.imshow('Lunge Counter with Auto-Detection', image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Print final statistics\n",
    "print(f\"\\nFinal Lunge Results:\")\n",
    "print(f\"Total Lunges: {total_counter}\")\n",
    "print(f\"Left Leg Lunges: {left_counter}\")\n",
    "print(f\"Right Leg Lunges: {right_counter}\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e923ba-8710-436e-9aee-db1176f43cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumping jack completed! Count: 1\n",
      "Jumping jack completed! Count: 2\n",
      "Jumping jack completed! Count: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Make detection\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m results \u001b[38;5;241m=\u001b[39m pose\u001b[38;5;241m.\u001b[39mprocess(image)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Recolor back to BGR\u001b[39;00m\n\u001b[0;32m    108\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mprocess(input_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image})\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mwait_until_idle()\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Function to calculate angle between three points\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calculate angle between three points\n",
    "    a, b, c are lists/arrays of [x, y] coordinates\n",
    "    b is the vertex point\n",
    "    \"\"\"\n",
    "    a = np.array(a)  # First point\n",
    "    b = np.array(b)  # Vertex point\n",
    "    c = np.array(c)  # End point\n",
    "    \n",
    "    # Calculate vectors\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    \n",
    "    # Calculate angle using dot product\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    \n",
    "    # Convert to degrees\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Function to calculate distance between two points\n",
    "def calculate_distance(point1, point2):\n",
    "    \"\"\"Calculate Euclidean distance between two points\"\"\"\n",
    "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "# Function to detect jumping jack stage\n",
    "def detect_jumping_jack_stage(left_wrist, right_wrist, left_ankle, right_ankle, \n",
    "                            left_shoulder, right_shoulder, left_hip, right_hip):\n",
    "    \"\"\"\n",
    "    Detect if person is in 'up' or 'down' stage of jumping jack\n",
    "    Up stage: arms raised, legs spread apart\n",
    "    Down stage: arms down, legs together\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate arm spread (distance between wrists relative to shoulders)\n",
    "    wrist_distance = calculate_distance(left_wrist, right_wrist)\n",
    "    shoulder_distance = calculate_distance(left_shoulder, right_shoulder)\n",
    "    arm_spread_ratio = wrist_distance / shoulder_distance if shoulder_distance > 0 else 0\n",
    "    \n",
    "    # Calculate leg spread (distance between ankles relative to hips)\n",
    "    ankle_distance = calculate_distance(left_ankle, right_ankle)\n",
    "    hip_distance = calculate_distance(left_hip, right_hip)\n",
    "    leg_spread_ratio = ankle_distance / hip_distance if hip_distance > 0 else 0\n",
    "    \n",
    "    # Calculate arm height (average wrist height relative to shoulders)\n",
    "    avg_wrist_y = (left_wrist[1] + right_wrist[1]) / 2\n",
    "    avg_shoulder_y = (left_shoulder[1] + right_shoulder[1]) / 2\n",
    "    arm_height_ratio = (avg_shoulder_y - avg_wrist_y)  # Positive when arms are raised\n",
    "    \n",
    "    # Thresholds for detection (adjust based on testing)\n",
    "    ARM_SPREAD_UP_THRESHOLD = 1.8    # Arms spread wide\n",
    "    ARM_SPREAD_DOWN_THRESHOLD = 1.2  # Arms close to body\n",
    "    LEG_SPREAD_UP_THRESHOLD = 1.8    # Legs spread apart\n",
    "    LEG_SPREAD_DOWN_THRESHOLD = 1.2  # Legs close together\n",
    "    ARM_HEIGHT_THRESHOLD = 0.1       # Arms raised above shoulders\n",
    "    \n",
    "    # Determine stage based on arm and leg positions\n",
    "    arms_up = (arm_spread_ratio > ARM_SPREAD_UP_THRESHOLD and arm_height_ratio > ARM_HEIGHT_THRESHOLD)\n",
    "    arms_down = (arm_spread_ratio < ARM_SPREAD_DOWN_THRESHOLD and arm_height_ratio < -ARM_HEIGHT_THRESHOLD)\n",
    "    \n",
    "    legs_apart = leg_spread_ratio > LEG_SPREAD_UP_THRESHOLD\n",
    "    legs_together = leg_spread_ratio < LEG_SPREAD_DOWN_THRESHOLD\n",
    "    \n",
    "    if arms_up and legs_apart:\n",
    "        return 'up', arm_spread_ratio, leg_spread_ratio, arm_height_ratio\n",
    "    elif arms_down and legs_together:\n",
    "        return 'down', arm_spread_ratio, leg_spread_ratio, arm_height_ratio\n",
    "    else:\n",
    "        return 'transition', arm_spread_ratio, leg_spread_ratio, arm_height_ratio\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Jumping jack counter variables\n",
    "jj_counter = 0\n",
    "jj_stage = None\n",
    "\n",
    "# Set up Mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "            \n",
    "        # Get frame dimensions\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Initialize variables\n",
    "        left_knee_angle = 0\n",
    "        right_knee_angle = 0\n",
    "        current_stage = 'none'\n",
    "        arm_spread_ratio = 0\n",
    "        leg_spread_ratio = 0\n",
    "        arm_height_ratio = 0\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                \n",
    "                # Get coordinates for required landmarks (normalized)\n",
    "                left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                               landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "                right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                                landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "                \n",
    "                left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "                right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "                \n",
    "                left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "                right_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "                \n",
    "                left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "                right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "                \n",
    "                left_knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "                right_knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "                \n",
    "                left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "                right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                              landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "                \n",
    "                # Calculate knee angles for both legs (hip-knee-ankle)\n",
    "                left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "                right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "                \n",
    "                # Detect jumping jack stage\n",
    "                current_stage, arm_spread_ratio, leg_spread_ratio, arm_height_ratio = detect_jumping_jack_stage(\n",
    "                    left_wrist, right_wrist, left_ankle, right_ankle,\n",
    "                    left_shoulder, right_shoulder, left_hip, right_hip\n",
    "                )\n",
    "                \n",
    "                # Jumping jack counting logic\n",
    "                if current_stage == 'up':\n",
    "                    jj_stage = 'up'\n",
    "                elif current_stage == 'down' and jj_stage == 'up':\n",
    "                    jj_stage = 'down'\n",
    "                    jj_counter += 1\n",
    "                    print(f\"Jumping jack completed! Count: {jj_counter}\")\n",
    "                \n",
    "                # Convert coordinates to pixel coordinates for visualization\n",
    "                left_wrist_coords = tuple(np.multiply(left_wrist, [frame_width, frame_height]).astype(int))\n",
    "                right_wrist_coords = tuple(np.multiply(right_wrist, [frame_width, frame_height]).astype(int))\n",
    "                left_ankle_coords = tuple(np.multiply(left_ankle, [frame_width, frame_height]).astype(int))\n",
    "                right_ankle_coords = tuple(np.multiply(right_ankle, [frame_width, frame_height]).astype(int))\n",
    "                left_knee_coords = tuple(np.multiply(left_knee, [frame_width, frame_height]).astype(int))\n",
    "                right_knee_coords = tuple(np.multiply(right_knee, [frame_width, frame_height]).astype(int))\n",
    "                \n",
    "                # Draw indicators based on current stage\n",
    "                if current_stage == 'up':\n",
    "                    # Highlight key points in green when in up position\n",
    "                    cv2.circle(image, left_wrist_coords, 8, (0, 255, 0), -1)\n",
    "                    cv2.circle(image, right_wrist_coords, 8, (0, 255, 0), -1)\n",
    "                    cv2.circle(image, left_ankle_coords, 8, (0, 255, 0), -1)\n",
    "                    cv2.circle(image, right_ankle_coords, 8, (0, 255, 0), -1)\n",
    "                elif current_stage == 'down':\n",
    "                    # Highlight key points in blue when in down position\n",
    "                    cv2.circle(image, left_wrist_coords, 8, (255, 0, 0), -1)\n",
    "                    cv2.circle(image, right_wrist_coords, 8, (255, 0, 0), -1)\n",
    "                    cv2.circle(image, left_ankle_coords, 8, (255, 0, 0), -1)\n",
    "                    cv2.circle(image, right_ankle_coords, 8, (255, 0, 0), -1)\n",
    "                else:\n",
    "                    # Neutral color during transition\n",
    "                    cv2.circle(image, left_wrist_coords, 6, (0, 255, 255), -1)\n",
    "                    cv2.circle(image, right_wrist_coords, 6, (0, 255, 255), -1)\n",
    "                    cv2.circle(image, left_ankle_coords, 6, (0, 255, 255), -1)\n",
    "                    cv2.circle(image, right_ankle_coords, 6, (0, 255, 255), -1)\n",
    "                \n",
    "                # Draw knee angle indicators\n",
    "                cv2.circle(image, left_knee_coords, 6, (255, 255, 0), -1)\n",
    "                cv2.circle(image, right_knee_coords, 6, (255, 255, 0), -1)\n",
    "                \n",
    "                # Display knee angles\n",
    "                cv2.putText(image, f'{int(left_knee_angle)}°',\n",
    "                           (left_knee_coords[0] - 30, left_knee_coords[1] - 15),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "                \n",
    "                cv2.putText(image, f'{int(right_knee_angle)}°',\n",
    "                           (right_knee_coords[0] + 10, right_knee_coords[1] - 15),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "        \n",
    "        # Setup main status display\n",
    "        cv2.rectangle(image, (10, 10), (500, 220), (50, 50, 50), -1)\n",
    "        cv2.rectangle(image, (10, 10), (500, 220), (255, 255, 255), 2)\n",
    "        \n",
    "        # Display jumping jack count\n",
    "        cv2.putText(image, 'JUMPING JACKS', (20, 40), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(jj_counter), (20, 80), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "        \n",
    "        # Display current stage\n",
    "        stage_color = (0, 255, 0) if current_stage == 'up' else (255, 0, 0) if current_stage == 'down' else (0, 255, 255)\n",
    "        cv2.putText(image, 'STAGE', (200, 40), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, current_stage.upper(), (200, 75), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, stage_color, 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display knee angles\n",
    "        cv2.putText(image, 'KNEE ANGLES', (350, 40), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'L: {int(left_knee_angle)}°', (350, 65), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'R: {int(right_knee_angle)}°', (350, 85), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display detection metrics\n",
    "        cv2.putText(image, 'DETECTION METRICS', (20, 115), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'Arm Spread: {arm_spread_ratio:.2f}', (20, 140), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'Leg Spread: {leg_spread_ratio:.2f}', (20, 160), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'Arm Height: {arm_height_ratio:.2f}', (20, 180), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Display current internal stage\n",
    "        internal_stage = jj_stage if jj_stage else 'None'\n",
    "        cv2.putText(image, f'Internal: {internal_stage}', (250, 140), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Render pose detections\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "        \n",
    "        # Add instructions\n",
    "        cv2.putText(image, 'Perform jumping jacks: Jump up with arms and legs spread, then back down. Press Q to quit.', \n",
    "                    (10, frame_height - 20), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Display the image\n",
    "        cv2.imshow('Jumping Jack Counter', image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Print final statistics\n",
    "print(f\"\\nFinal Jumping Jack Results:\")\n",
    "print(f\"Total Jumping Jacks: {jj_counter}\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec47e34-1148-4043-a464-5ea0a61e3640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push-up completed! Count: 1\n",
      "Push-up completed! Count: 2\n",
      "Push-up completed! Count: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 374\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;66;03m# Display the image\u001b[39;00m\n\u001b[0;32m    372\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPush-up Counter with Smoothing\u001b[39m\u001b[38;5;124m'\u001b[39m, image)\n\u001b[1;32m--> 374\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    375\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# Print final statistics\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Function to calculate angle between three points\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calculate angle between three points\n",
    "    a, b, c are lists/arrays of [x, y] coordinates\n",
    "    b is the vertex point\n",
    "    \"\"\"\n",
    "    a = np.array(a)  # First point\n",
    "    b = np.array(b)  # Vertex point\n",
    "    c = np.array(c)  # End point\n",
    "    \n",
    "    # Calculate vectors\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    \n",
    "    # Calculate angle using dot product\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    \n",
    "    # Convert to degrees\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Function to calculate distance between two points\n",
    "def calculate_distance(point1, point2):\n",
    "    \"\"\"Calculate Euclidean distance between two points\"\"\"\n",
    "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "# Function to detect push-up stage with improved logic\n",
    "def detect_pushup_stage(left_elbow_angle, right_elbow_angle, body_straightness, \n",
    "                       left_shoulder, right_shoulder, left_hip, right_hip):\n",
    "    \"\"\"\n",
    "    Detect if person is in 'up' or 'down' stage of push-up\n",
    "    Down stage: elbows bent (smaller angles), body straight and low\n",
    "    Up stage: elbows extended (larger angles), body straight and high\n",
    "    \"\"\"\n",
    "    \n",
    "    # Average elbow angle for more stable detection\n",
    "    avg_elbow_angle = (left_elbow_angle + right_elbow_angle) / 2\n",
    "    \n",
    "    # Calculate body height (average shoulder height)\n",
    "    avg_shoulder_y = (left_shoulder[1] + right_shoulder[1]) / 2\n",
    "    avg_hip_y = (left_hip[1] + right_hip[1]) / 2\n",
    "    \n",
    "    # Thresholds for push-up detection\n",
    "    DOWN_ELBOW_THRESHOLD = 90   # Elbows bent position\n",
    "    UP_ELBOW_THRESHOLD = 140    # Elbows extended position\n",
    "    BODY_STRAIGHT_THRESHOLD = 0.15  # Body alignment threshold\n",
    "    \n",
    "    # Check body straightness (shoulders-hips alignment)\n",
    "    is_body_straight = body_straightness < BODY_STRAIGHT_THRESHOLD\n",
    "    \n",
    "    # Determine stage based on elbow angles and body position\n",
    "    if avg_elbow_angle < DOWN_ELBOW_THRESHOLD and is_body_straight:\n",
    "        return 'down'\n",
    "    elif avg_elbow_angle > UP_ELBOW_THRESHOLD and is_body_straight:\n",
    "        return 'up'\n",
    "    else:\n",
    "        return 'transition'\n",
    "\n",
    "# Function to calculate body straightness\n",
    "def calculate_body_straightness(left_shoulder, right_shoulder, left_hip, right_hip, \n",
    "                              left_knee, right_knee, left_ankle, right_ankle):\n",
    "    \"\"\"\n",
    "    Calculate how straight the body is from shoulders to ankles\n",
    "    Returns a value where lower means straighter\n",
    "    \"\"\"\n",
    "    # Calculate midpoints\n",
    "    shoulder_mid = [(left_shoulder[0] + right_shoulder[0]) / 2, \n",
    "                   (left_shoulder[1] + right_shoulder[1]) / 2]\n",
    "    hip_mid = [(left_hip[0] + right_hip[0]) / 2, \n",
    "               (left_hip[1] + right_hip[1]) / 2]\n",
    "    knee_mid = [(left_knee[0] + right_knee[0]) / 2, \n",
    "                (left_knee[1] + right_knee[1]) / 2]\n",
    "    ankle_mid = [(left_ankle[0] + right_ankle[0]) / 2, \n",
    "                 (left_ankle[1] + right_ankle[1]) / 2]\n",
    "    \n",
    "    # Calculate angles between body segments\n",
    "    shoulder_hip_knee_angle = calculate_angle(shoulder_mid, hip_mid, knee_mid)\n",
    "    hip_knee_ankle_angle = calculate_angle(hip_mid, knee_mid, ankle_mid)\n",
    "    \n",
    "    # Deviation from straight line (180 degrees)\n",
    "    straightness_score = abs(180 - shoulder_hip_knee_angle) + abs(180 - hip_knee_ankle_angle)\n",
    "    \n",
    "    return straightness_score / 360.0  # Normalize to 0-1 range\n",
    "\n",
    "# Smoothing class for stable detection\n",
    "class SmoothingFilter:\n",
    "    def __init__(self, window_size=5):\n",
    "        self.window_size = window_size\n",
    "        self.values = deque(maxlen=window_size)\n",
    "    \n",
    "    def add_value(self, value):\n",
    "        self.values.append(value)\n",
    "    \n",
    "    def get_smoothed_value(self):\n",
    "        if len(self.values) == 0:\n",
    "            return 0\n",
    "        return sum(self.values) / len(self.values)\n",
    "    \n",
    "    def get_trend(self):\n",
    "        if len(self.values) < 2:\n",
    "            return 'stable'\n",
    "        recent = sum(list(self.values)[-3:]) / min(3, len(self.values))\n",
    "        older = sum(list(self.values)[:-3]) / max(1, len(self.values) - 3)\n",
    "        if recent > older + 5:\n",
    "            return 'increasing'\n",
    "        elif recent < older - 5:\n",
    "            return 'decreasing'\n",
    "        else:\n",
    "            return 'stable'\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Push-up counter variables\n",
    "pushup_counter = 0\n",
    "pushup_stage = None\n",
    "last_stage = None\n",
    "\n",
    "# Smoothing filters\n",
    "left_elbow_filter = SmoothingFilter(window_size=7)\n",
    "right_elbow_filter = SmoothingFilter(window_size=7)\n",
    "body_straightness_filter = SmoothingFilter(window_size=5)\n",
    "\n",
    "# Stage transition tracking for robust counting\n",
    "stage_history = deque(maxlen=10)\n",
    "min_stage_duration = 3  # Minimum frames to confirm stage change\n",
    "\n",
    "# Set up Mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "            \n",
    "        # Get frame dimensions\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Initialize variables\n",
    "        left_elbow_angle = 0\n",
    "        right_elbow_angle = 0\n",
    "        left_hip_angle = 0\n",
    "        right_hip_angle = 0\n",
    "        body_straightness = 1.0\n",
    "        current_stage = 'none'\n",
    "        smoothed_left_elbow = 0\n",
    "        smoothed_right_elbow = 0\n",
    "        avg_elbow_angle = 0\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                \n",
    "                # Get coordinates for required landmarks (normalized)\n",
    "                left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                               landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "                right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                                landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "                \n",
    "                left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "                right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "                \n",
    "                left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "                right_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "                \n",
    "                left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "                right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "                \n",
    "                left_knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "                right_knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "                \n",
    "                left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "                right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                              landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "                \n",
    "                # Calculate joint angles\n",
    "                # Elbow angles (shoulder-elbow-wrist)\n",
    "                left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "                right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "                \n",
    "                # Hip angles (shoulder-hip-knee) for body posture\n",
    "                left_hip_angle = calculate_angle(left_shoulder, left_hip, left_knee)\n",
    "                right_hip_angle = calculate_angle(right_shoulder, right_hip, right_knee)\n",
    "                \n",
    "                # Calculate body straightness\n",
    "                body_straightness = calculate_body_straightness(\n",
    "                    left_shoulder, right_shoulder, left_hip, right_hip,\n",
    "                    left_knee, right_knee, left_ankle, right_ankle\n",
    "                )\n",
    "                \n",
    "                # Apply smoothing to elbow angles\n",
    "                left_elbow_filter.add_value(left_elbow_angle)\n",
    "                right_elbow_filter.add_value(right_elbow_angle)\n",
    "                body_straightness_filter.add_value(body_straightness)\n",
    "                \n",
    "                smoothed_left_elbow = left_elbow_filter.get_smoothed_value()\n",
    "                smoothed_right_elbow = right_elbow_filter.get_smoothed_value()\n",
    "                smoothed_body_straightness = body_straightness_filter.get_smoothed_value()\n",
    "                avg_elbow_angle = (smoothed_left_elbow + smoothed_right_elbow) / 2\n",
    "                \n",
    "                # Detect push-up stage using smoothed values\n",
    "                current_stage = detect_pushup_stage(\n",
    "                    smoothed_left_elbow, smoothed_right_elbow, \n",
    "                    smoothed_body_straightness, left_shoulder, right_shoulder, \n",
    "                    left_hip, right_hip\n",
    "                )\n",
    "                \n",
    "                # Add to stage history for robust detection\n",
    "                stage_history.append(current_stage)\n",
    "                \n",
    "                # Count stages only if consistent over multiple frames\n",
    "                if len(stage_history) >= min_stage_duration:\n",
    "                    recent_stages = list(stage_history)[-min_stage_duration:]\n",
    "                    if all(stage == current_stage for stage in recent_stages):\n",
    "                        # Stage is consistent, update push-up logic\n",
    "                        if current_stage == 'up' and pushup_stage != 'up':\n",
    "                            pushup_stage = 'up'\n",
    "                        elif current_stage == 'down' and pushup_stage == 'up':\n",
    "                            pushup_stage = 'down'\n",
    "                            pushup_counter += 1\n",
    "                            print(f\"Push-up completed! Count: {pushup_counter}\")\n",
    "                \n",
    "                # Convert coordinates to pixel coordinates for visualization\n",
    "                left_elbow_coords = tuple(np.multiply(left_elbow, [frame_width, frame_height]).astype(int))\n",
    "                right_elbow_coords = tuple(np.multiply(right_elbow, [frame_width, frame_height]).astype(int))\n",
    "                left_wrist_coords = tuple(np.multiply(left_wrist, [frame_width, frame_height]).astype(int))\n",
    "                right_wrist_coords = tuple(np.multiply(right_wrist, [frame_width, frame_height]).astype(int))\n",
    "                left_shoulder_coords = tuple(np.multiply(left_shoulder, [frame_width, frame_height]).astype(int))\n",
    "                right_shoulder_coords = tuple(np.multiply(right_shoulder, [frame_width, frame_height]).astype(int))\n",
    "                \n",
    "                # Draw indicators based on current stage\n",
    "                if current_stage == 'down':\n",
    "                    # Highlight key points in red when in down position\n",
    "                    cv2.circle(image, left_elbow_coords, 10, (0, 0, 255), -1)\n",
    "                    cv2.circle(image, right_elbow_coords, 10, (0, 0, 255), -1)\n",
    "                    cv2.circle(image, left_wrist_coords, 6, (0, 0, 255), -1)\n",
    "                    cv2.circle(image, right_wrist_coords, 6, (0, 0, 255), -1)\n",
    "                elif current_stage == 'up':\n",
    "                    # Highlight key points in green when in up position\n",
    "                    cv2.circle(image, left_elbow_coords, 10, (0, 255, 0), -1)\n",
    "                    cv2.circle(image, right_elbow_coords, 10, (0, 255, 0), -1)\n",
    "                    cv2.circle(image, left_wrist_coords, 6, (0, 255, 0), -1)\n",
    "                    cv2.circle(image, right_wrist_coords, 6, (0, 255, 0), -1)\n",
    "                else:\n",
    "                    # Neutral color during transition\n",
    "                    cv2.circle(image, left_elbow_coords, 8, (0, 255, 255), -1)\n",
    "                    cv2.circle(image, right_elbow_coords, 8, (0, 255, 255), -1)\n",
    "                    cv2.circle(image, left_wrist_coords, 4, (0, 255, 255), -1)\n",
    "                    cv2.circle(image, right_wrist_coords, 4, (0, 255, 255), -1)\n",
    "                \n",
    "                # Draw shoulder indicators\n",
    "                cv2.circle(image, left_shoulder_coords, 6, (255, 255, 0), -1)\n",
    "                cv2.circle(image, right_shoulder_coords, 6, (255, 255, 0), -1)\n",
    "                \n",
    "                # Display elbow angles\n",
    "                cv2.putText(image, f'{int(smoothed_left_elbow)}°',\n",
    "                           (left_elbow_coords[0] - 40, left_elbow_coords[1] - 20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                cv2.putText(image, f'{int(smoothed_right_elbow)}°',\n",
    "                           (right_elbow_coords[0] + 10, right_elbow_coords[1] - 20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "        \n",
    "        # Setup main status display\n",
    "        cv2.rectangle(image, (10, 10), (550, 280), (50, 50, 50), -1)\n",
    "        cv2.rectangle(image, (10, 10), (550, 280), (255, 255, 255), 2)\n",
    "        \n",
    "        # Display push-up count\n",
    "        cv2.putText(image, 'PUSH-UPS', (20, 45), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(pushup_counter), (20, 90), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2.5, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "        \n",
    "        # Display current stage\n",
    "        stage_color = (0, 255, 0) if current_stage == 'up' else (0, 0, 255) if current_stage == 'down' else (0, 255, 255)\n",
    "        cv2.putText(image, 'STAGE', (200, 45), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, current_stage.upper(), (200, 80), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, stage_color, 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display elbow angles\n",
    "        cv2.putText(image, 'ELBOW ANGLES', (350, 45), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'L: {int(smoothed_left_elbow)}°', (350, 70), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'R: {int(smoothed_right_elbow)}°', (350, 95), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'Avg: {int(avg_elbow_angle)}°', (450, 82), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display hip angles\n",
    "        cv2.putText(image, 'HIP ANGLES', (20, 130), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'L: {int(left_hip_angle)}°', (20, 155), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'R: {int(right_hip_angle)}°', (120, 155), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display body metrics\n",
    "        cv2.putText(image, 'BODY METRICS', (250, 130), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        straightness_color = (0, 255, 0) if smoothed_body_straightness < 0.15 else (255, 255, 0) if smoothed_body_straightness < 0.3 else (0, 0, 255)\n",
    "        cv2.putText(image, f'Straightness: {smoothed_body_straightness:.2f}', (250, 155), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, straightness_color, 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Display internal stage for debugging\n",
    "        internal_stage = pushup_stage if pushup_stage else 'None'\n",
    "        cv2.putText(image, f'Internal: {internal_stage}', (250, 175), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Display angle trends\n",
    "        left_trend = left_elbow_filter.get_trend()\n",
    "        right_trend = right_elbow_filter.get_trend()\n",
    "        cv2.putText(image, f'L Trend: {left_trend}', (20, 200), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (180, 180, 180), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, f'R Trend: {right_trend}', (20, 220), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (180, 180, 180), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Display smoothing status\n",
    "        cv2.putText(image, f'Smooth Frames: {len(stage_history)}/{min_stage_duration}', (20, 250), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (150, 150, 150), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Render pose detections\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "        \n",
    "        # Add instructions\n",
    "        cv2.putText(image, 'Perform push-ups: Keep body straight, lower chest to ground, push back up. Press Q to quit.', \n",
    "                    (10, frame_height - 20), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Display the image\n",
    "        cv2.imshow('Push-up Counter with Smoothing', image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Print final statistics\n",
    "print(f\"\\nFinal Push-up Results:\")\n",
    "print(f\"Total Push-ups: {pushup_counter}\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e98eb7-1d09-4e99-b6de-2af66663a35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Fitness Tracker Started!\n",
      "👍 Thumbs up: Resume timer\n",
      "✌️ Peace sign: Pause timer\n",
      "🏋️ Perform squats to count reps (ONLY WHEN TIMER IS RUNNING)\n",
      "Press 'q' to quit, 'r' to reset both timer and counter\n",
      "Timer RESUMED (👍) - Squats will now be counted!\n",
      "Timer PAUSED (✌️) at: 00:00:07 - Squats will NOT be counted while paused\n",
      "Timer RESUMED (👍) - Squats will now be counted!\n",
      "Timer PAUSED (✌️) at: 00:00:13 - Squats will NOT be counted while paused\n",
      "Timer RESUMED (👍) - Squats will now be counted!\n",
      "Squat count: 1 (Timer: 00:00:15)\n",
      "Squat count: 2 (Timer: 00:00:16)\n",
      "Timer PAUSED (✌️) at: 00:00:17 - Squats will NOT be counted while paused\n",
      "Timer RESUMED (👍) - Squats will now be counted!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 289\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# Process the frame for both hand and pose detection\u001b[39;00m\n\u001b[0;32m    288\u001b[0m hand_results \u001b[38;5;241m=\u001b[39m hands\u001b[38;5;241m.\u001b[39mprocess(image)\n\u001b[1;32m--> 289\u001b[0m pose_results \u001b[38;5;241m=\u001b[39m pose\u001b[38;5;241m.\u001b[39mprocess(image)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m# Convert back to BGR\u001b[39;00m\n\u001b[0;32m    292\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mprocess(input_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image})\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mwait_until_idle()\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def calculate_distance(point1, point2):\n",
    "    \"\"\"Calculate Euclidean distance between two points\"\"\"\n",
    "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calculate angle between three points\n",
    "    a, b, c are lists/arrays of [x, y] coordinates\n",
    "    b is the vertex point (knee in this case)\n",
    "    \"\"\"\n",
    "    a = np.array(a)  # First point (hip)\n",
    "    b = np.array(b)  # Vertex point (knee)\n",
    "    c = np.array(c)  # End point (ankle)\n",
    "    \n",
    "    # Calculate vectors\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    \n",
    "    # Calculate angle using dot product\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    \n",
    "    # Convert to degrees\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def is_finger_extended(tip, pip, mcp, wrist):\n",
    "    \"\"\"\n",
    "    Check if a finger is extended by comparing distances\n",
    "    More robust approach that works from multiple angles\n",
    "    \"\"\"\n",
    "    # Distance from wrist to tip\n",
    "    tip_dist = calculate_distance(tip, wrist)\n",
    "    # Distance from wrist to middle joint\n",
    "    pip_dist = calculate_distance(pip, wrist)\n",
    "    # Distance from wrist to base\n",
    "    mcp_dist = calculate_distance(mcp, wrist)\n",
    "    \n",
    "    # Finger is extended if tip is farther from wrist than middle joint\n",
    "    # and middle joint is farther than base joint\n",
    "    return tip_dist > pip_dist and pip_dist > mcp_dist * 0.8\n",
    "\n",
    "def is_thumb_extended(thumb_tip, thumb_ip, thumb_mcp, wrist):\n",
    "    \"\"\"\n",
    "    Special case for thumb detection - more lenient\n",
    "    \"\"\"\n",
    "    # Distance from wrist to thumb tip\n",
    "    tip_dist = calculate_distance(thumb_tip, wrist)\n",
    "    # Distance from wrist to thumb middle joint\n",
    "    ip_dist = calculate_distance(thumb_ip, wrist)\n",
    "    # Distance from wrist to thumb base\n",
    "    mcp_dist = calculate_distance(thumb_mcp, wrist)\n",
    "    \n",
    "    # Thumb is extended if tip is farther from wrist than middle joint\n",
    "    return tip_dist > ip_dist * 0.9 and tip_dist > mcp_dist * 0.7\n",
    "\n",
    "def detect_thumbs_up_robust(landmarks):\n",
    "    \"\"\"\n",
    "    Enhanced thumbs up detection that works from multiple angles\n",
    "    \"\"\"\n",
    "    # Landmark indices\n",
    "    wrist = 0\n",
    "    thumb_tip = 4\n",
    "    thumb_ip = 3\n",
    "    thumb_mcp = 2\n",
    "    index_tip = 8\n",
    "    index_pip = 6\n",
    "    index_mcp = 5\n",
    "    middle_tip = 12\n",
    "    middle_pip = 10\n",
    "    middle_mcp = 9\n",
    "    ring_tip = 16\n",
    "    ring_pip = 14\n",
    "    ring_mcp = 13\n",
    "    pinky_tip = 20\n",
    "    pinky_pip = 18\n",
    "    pinky_mcp = 17\n",
    "    \n",
    "    # Get coordinates\n",
    "    wrist_coords = [landmarks[wrist].x, landmarks[wrist].y]\n",
    "    thumb_tip_coords = [landmarks[thumb_tip].x, landmarks[thumb_tip].y]\n",
    "    thumb_ip_coords = [landmarks[thumb_ip].x, landmarks[thumb_ip].y]\n",
    "    thumb_mcp_coords = [landmarks[thumb_mcp].x, landmarks[thumb_mcp].y]\n",
    "    \n",
    "    index_tip_coords = [landmarks[index_tip].x, landmarks[index_tip].y]\n",
    "    index_pip_coords = [landmarks[index_pip].x, landmarks[index_pip].y]\n",
    "    index_mcp_coords = [landmarks[index_mcp].x, landmarks[index_mcp].y]\n",
    "    \n",
    "    middle_tip_coords = [landmarks[middle_tip].x, landmarks[middle_tip].y]\n",
    "    middle_pip_coords = [landmarks[middle_pip].x, landmarks[middle_pip].y]\n",
    "    middle_mcp_coords = [landmarks[middle_mcp].x, landmarks[middle_mcp].y]\n",
    "    \n",
    "    ring_tip_coords = [landmarks[ring_tip].x, landmarks[ring_tip].y]\n",
    "    ring_pip_coords = [landmarks[ring_pip].x, landmarks[ring_pip].y]\n",
    "    ring_mcp_coords = [landmarks[ring_mcp].x, landmarks[ring_mcp].y]\n",
    "    \n",
    "    pinky_tip_coords = [landmarks[pinky_tip].x, landmarks[pinky_tip].y]\n",
    "    pinky_pip_coords = [landmarks[pinky_pip].x, landmarks[pinky_pip].y]\n",
    "    pinky_mcp_coords = [landmarks[pinky_mcp].x, landmarks[pinky_mcp].y]\n",
    "    \n",
    "    # Check if thumb is extended\n",
    "    thumb_extended = is_thumb_extended(thumb_tip_coords, thumb_ip_coords, thumb_mcp_coords, wrist_coords)\n",
    "    \n",
    "    # Check if other fingers are NOT extended (folded)\n",
    "    index_folded = not is_finger_extended(index_tip_coords, index_pip_coords, index_mcp_coords, wrist_coords)\n",
    "    middle_folded = not is_finger_extended(middle_tip_coords, middle_pip_coords, middle_mcp_coords, wrist_coords)\n",
    "    ring_folded = not is_finger_extended(ring_tip_coords, ring_pip_coords, ring_mcp_coords, wrist_coords)\n",
    "    pinky_folded = not is_finger_extended(pinky_tip_coords, pinky_pip_coords, pinky_mcp_coords, wrist_coords)\n",
    "    \n",
    "    # Additional check: thumb should be reasonably separated from palm center\n",
    "    palm_center = [\n",
    "        (landmarks[0].x + landmarks[5].x + landmarks[9].x + landmarks[13].x + landmarks[17].x) / 5,\n",
    "        (landmarks[0].y + landmarks[5].y + landmarks[9].y + landmarks[13].y + landmarks[17].y) / 5\n",
    "    ]\n",
    "    thumb_separated = calculate_distance(thumb_tip_coords, palm_center) > 0.08\n",
    "    \n",
    "    # Count how many conditions are met (more flexible approach)\n",
    "    conditions_met = sum([\n",
    "        thumb_extended,\n",
    "        thumb_separated,\n",
    "        index_folded,\n",
    "        middle_folded,\n",
    "        ring_folded,\n",
    "        pinky_folded\n",
    "    ])\n",
    "    \n",
    "    # Require at least 4 out of 6 conditions to be met\n",
    "    return conditions_met >= 4\n",
    "\n",
    "def detect_peace_sign_robust(landmarks):\n",
    "    \"\"\"\n",
    "    Enhanced peace sign detection that works from multiple angles\n",
    "    \"\"\"\n",
    "    # Landmark indices\n",
    "    wrist = 0\n",
    "    thumb_tip = 4\n",
    "    thumb_pip = 3\n",
    "    thumb_mcp = 2\n",
    "    index_tip = 8\n",
    "    index_pip = 6\n",
    "    index_mcp = 5\n",
    "    middle_tip = 12\n",
    "    middle_pip = 10\n",
    "    middle_mcp = 9\n",
    "    ring_tip = 16\n",
    "    ring_pip = 14\n",
    "    ring_mcp = 13\n",
    "    pinky_tip = 20\n",
    "    pinky_pip = 18\n",
    "    pinky_mcp = 17\n",
    "    \n",
    "    # Get coordinates\n",
    "    wrist_coords = [landmarks[wrist].x, landmarks[wrist].y]\n",
    "    \n",
    "    index_tip_coords = [landmarks[index_tip].x, landmarks[index_tip].y]\n",
    "    index_pip_coords = [landmarks[index_pip].x, landmarks[index_pip].y]\n",
    "    index_mcp_coords = [landmarks[index_mcp].x, landmarks[index_mcp].y]\n",
    "    \n",
    "    middle_tip_coords = [landmarks[middle_tip].x, landmarks[middle_tip].y]\n",
    "    middle_pip_coords = [landmarks[middle_pip].x, landmarks[middle_pip].y]\n",
    "    middle_mcp_coords = [landmarks[middle_mcp].x, landmarks[middle_mcp].y]\n",
    "    \n",
    "    thumb_tip_coords = [landmarks[thumb_tip].x, landmarks[thumb_tip].y]\n",
    "    thumb_pip_coords = [landmarks[thumb_pip].x, landmarks[thumb_pip].y]\n",
    "    thumb_mcp_coords = [landmarks[thumb_mcp].x, landmarks[thumb_mcp].y]\n",
    "    \n",
    "    ring_tip_coords = [landmarks[ring_tip].x, landmarks[ring_tip].y]\n",
    "    ring_pip_coords = [landmarks[ring_pip].x, landmarks[ring_pip].y]\n",
    "    ring_mcp_coords = [landmarks[ring_mcp].x, landmarks[ring_mcp].y]\n",
    "    \n",
    "    pinky_tip_coords = [landmarks[pinky_tip].x, landmarks[pinky_tip].y]\n",
    "    pinky_pip_coords = [landmarks[pinky_pip].x, landmarks[pinky_pip].y]\n",
    "    pinky_mcp_coords = [landmarks[pinky_mcp].x, landmarks[pinky_mcp].y]\n",
    "    \n",
    "    # Check if index and middle fingers are extended\n",
    "    index_extended = is_finger_extended(index_tip_coords, index_pip_coords, index_mcp_coords, wrist_coords)\n",
    "    middle_extended = is_finger_extended(middle_tip_coords, middle_pip_coords, middle_mcp_coords, wrist_coords)\n",
    "    \n",
    "    # Check if thumb, ring, and pinky are NOT extended (folded)\n",
    "    thumb_folded = not is_thumb_extended(thumb_tip_coords, thumb_pip_coords, thumb_mcp_coords, wrist_coords)\n",
    "    ring_folded = not is_finger_extended(ring_tip_coords, ring_pip_coords, ring_mcp_coords, wrist_coords)\n",
    "    pinky_folded = not is_finger_extended(pinky_tip_coords, pinky_pip_coords, pinky_mcp_coords, wrist_coords)\n",
    "    \n",
    "    # Check that index and middle fingers are reasonably separated\n",
    "    fingers_separated = calculate_distance(index_tip_coords, middle_tip_coords) > 0.04\n",
    "    \n",
    "    # Alternative separation check using angles\n",
    "    palm_center = [\n",
    "        (landmarks[0].x + landmarks[5].x + landmarks[9].x + landmarks[13].x + landmarks[17].x) / 5,\n",
    "        (landmarks[0].y + landmarks[5].y + landmarks[9].y + landmarks[13].y + landmarks[17].y) / 5\n",
    "    ]\n",
    "    \n",
    "    # Check if both fingers are extended away from palm\n",
    "    index_from_center = calculate_distance(index_tip_coords, palm_center) > 0.12\n",
    "    middle_from_center = calculate_distance(middle_tip_coords, palm_center) > 0.12\n",
    "    \n",
    "    # Count conditions met\n",
    "    conditions_met = sum([\n",
    "        index_extended,\n",
    "        middle_extended,\n",
    "        fingers_separated,\n",
    "        index_from_center,\n",
    "        middle_from_center,\n",
    "        thumb_folded,\n",
    "        ring_folded,\n",
    "        pinky_folded\n",
    "    ])\n",
    "    \n",
    "    # Require at least 5 out of 8 conditions to be met\n",
    "    return conditions_met >= 5\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format seconds into HH:MM:SS format\"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Timer variables\n",
    "timer_running = False\n",
    "start_time = 0\n",
    "elapsed_time = 0\n",
    "total_elapsed = 0\n",
    "\n",
    "# Gesture detection variables with improved settings\n",
    "last_gesture_time = 0\n",
    "gesture_cooldown = 1.0  # Reduced cooldown for better responsiveness\n",
    "gesture_hold_time = 0.2  # Reduced hold time for quicker detection\n",
    "current_gesture = None\n",
    "gesture_start_time = 0\n",
    "gesture_confirmed = False\n",
    "\n",
    "# Gesture stability tracking\n",
    "gesture_history = []\n",
    "gesture_history_size = 5  # Track last 5 frames\n",
    "\n",
    "# Squat counter variables\n",
    "squat_counter = 0\n",
    "squat_stage = None\n",
    "\n",
    "print(\"Enhanced Fitness Tracker Started!\")\n",
    "print(\"👍 Thumbs up: Resume timer\")\n",
    "print(\"✌️ Peace sign: Pause timer\")\n",
    "print(\"🏋️ Perform squats to count reps (ONLY WHEN TIMER IS RUNNING)\")\n",
    "print(\"Press 'q' to quit, 'r' to reset both timer and counter\")\n",
    "\n",
    "# Set up MediaPipe instances with improved settings\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=1,  # Increased complexity for better detection\n",
    "    min_detection_confidence=0.6,  # Lowered for easier detection\n",
    "    min_tracking_confidence=0.6,   # Lowered for better tracking\n",
    "    max_num_hands=2  # Allow both hands for more flexibility\n",
    ") as hands, mp_pose.Pose(\n",
    "    min_detection_confidence=0.5, \n",
    "    min_tracking_confidence=0.5\n",
    ") as pose:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "            \n",
    "        # Get frame dimensions\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        \n",
    "        # Flip frame horizontally for mirror effect\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Process the frame for both hand and pose detection\n",
    "        hand_results = hands.process(image)\n",
    "        pose_results = pose.process(image)\n",
    "        \n",
    "        # Convert back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        current_time = time.time()\n",
    "        detected_gesture = None\n",
    "        \n",
    "        # ===== ENHANCED HAND GESTURE PROCESSING =====\n",
    "        if hand_results.multi_hand_landmarks and hand_results.multi_handedness:\n",
    "            gesture_votes = {\"thumbs_up\": 0, \"peace_sign\": 0}\n",
    "            \n",
    "            for hand_landmarks, handedness in zip(hand_results.multi_hand_landmarks, hand_results.multi_handedness):\n",
    "                # Get hand label\n",
    "                hand_label = handedness.classification[0].label\n",
    "                \n",
    "                # Detect gestures with enhanced functions\n",
    "                thumbs_up = detect_thumbs_up_robust(hand_landmarks.landmark)\n",
    "                peace_sign = detect_peace_sign_robust(hand_landmarks.landmark)\n",
    "                \n",
    "                # Vote for gestures\n",
    "                if thumbs_up and not peace_sign:\n",
    "                    gesture_votes[\"thumbs_up\"] += 1\n",
    "                elif peace_sign and not thumbs_up:\n",
    "                    gesture_votes[\"peace_sign\"] += 1\n",
    "                \n",
    "                # Get hand center for visual feedback\n",
    "                wrist = hand_landmarks.landmark[0]\n",
    "                center_coords = tuple(np.multiply([wrist.x, wrist.y], \n",
    "                                                [frame_width, frame_height]).astype(int))\n",
    "                \n",
    "                # Visual feedback for hand gestures\n",
    "                if thumbs_up and not peace_sign:\n",
    "                    cv2.circle(image, center_coords, 30, (0, 255, 0), 4)\n",
    "                    cv2.putText(image, f\"👍 {hand_label}\", \n",
    "                               (center_coords[0] - 70, center_coords[1] - 40),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                elif peace_sign and not thumbs_up:\n",
    "                    cv2.circle(image, center_coords, 30, (0, 0, 255), 4)\n",
    "                    cv2.putText(image, f\"✌️ {hand_label}\", \n",
    "                               (center_coords[0] - 70, center_coords[1] - 40),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                elif thumbs_up or peace_sign:\n",
    "                    # Show partial detection\n",
    "                    cv2.circle(image, center_coords, 20, (255, 255, 0), 2)\n",
    "                    cv2.putText(image, f\"? {hand_label}\", \n",
    "                               (center_coords[0] - 40, center_coords[1] - 30),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "                \n",
    "                # Draw hand landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=2, circle_radius=2),\n",
    "                    mp_drawing.DrawingSpec(color=(255, 0, 255), thickness=2)\n",
    "                )\n",
    "            \n",
    "            # Determine winning gesture\n",
    "            if gesture_votes[\"thumbs_up\"] > 0:\n",
    "                detected_gesture = \"thumbs_up\"\n",
    "            elif gesture_votes[\"peace_sign\"] > 0:\n",
    "                detected_gesture = \"peace_sign\"\n",
    "        \n",
    "        # ===== GESTURE HISTORY TRACKING FOR STABILITY =====\n",
    "        gesture_history.append(detected_gesture)\n",
    "        if len(gesture_history) > gesture_history_size:\n",
    "            gesture_history.pop(0)\n",
    "        \n",
    "        # Use majority vote from recent history for stability\n",
    "        if len(gesture_history) >= 3:\n",
    "            gesture_counts = {}\n",
    "            for g in gesture_history:\n",
    "                if g:\n",
    "                    gesture_counts[g] = gesture_counts.get(g, 0) + 1\n",
    "            \n",
    "            if gesture_counts:\n",
    "                stable_gesture = max(gesture_counts.items(), key=lambda x: x[1])[0]\n",
    "                if gesture_counts[stable_gesture] >= 2:  # At least 2 out of last few frames\n",
    "                    detected_gesture = stable_gesture\n",
    "        \n",
    "        # ===== POSE PROCESSING FOR SQUATS (ONLY WHEN TIMER IS RUNNING) =====\n",
    "        try:\n",
    "            if pose_results.pose_landmarks:\n",
    "                landmarks = pose_results.pose_landmarks.landmark\n",
    "                \n",
    "                # Get coordinates for left leg (normalized)\n",
    "                left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "                left_knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "                left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "                \n",
    "                # Get coordinates for right leg (normalized)\n",
    "                right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "                right_knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "                right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                              landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "                \n",
    "                # Calculate angles for both knees\n",
    "                left_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "                right_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "                \n",
    "                # Use average of both knee angles for squat detection\n",
    "                avg_angle = (left_angle + right_angle) / 2\n",
    "                \n",
    "                # Convert knee coordinates to pixel coordinates for display\n",
    "                left_knee_coords = tuple(np.multiply(left_knee, [frame_width, frame_height]).astype(int))\n",
    "                right_knee_coords = tuple(np.multiply(right_knee, [frame_width, frame_height]).astype(int))\n",
    "                \n",
    "                # Draw circles and put angle text at both knees\n",
    "                # Color changes based on timer status\n",
    "                knee_color = (0, 255, 0) if timer_running else (128, 128, 128)\n",
    "                cv2.circle(image, left_knee_coords, 8, knee_color, -1)\n",
    "                cv2.putText(image, f'{int(left_angle)}°',\n",
    "                           (left_knee_coords[0] - 40, left_knee_coords[1] - 20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                cv2.circle(image, right_knee_coords, 8, knee_color, -1)\n",
    "                cv2.putText(image, f'{int(right_angle)}°',\n",
    "                           (right_knee_coords[0] + 15, right_knee_coords[1] - 20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                # *** MODIFIED: Squat counter logic ONLY when timer is running ***\n",
    "                if timer_running:\n",
    "                    if avg_angle > 160:\n",
    "                        squat_stage = 'up'\n",
    "                    \n",
    "                    if avg_angle < 90 and squat_stage == 'up':\n",
    "                        squat_stage = 'down'\n",
    "                        squat_counter += 1\n",
    "                        print(f\"Squat count: {squat_counter} (Timer: {format_time(elapsed_time)})\")\n",
    "                else:\n",
    "                    # When timer is paused, don't update squat stage but keep tracking for display\n",
    "                    if avg_angle > 160:\n",
    "                        squat_stage = 'up (paused)'\n",
    "                    elif avg_angle < 90:\n",
    "                        squat_stage = 'down (paused)'\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\"Pose detection error:\", e)\n",
    "        \n",
    "        # ===== ENHANCED GESTURE TIMER LOGIC =====\n",
    "        if detected_gesture:\n",
    "            if current_gesture != detected_gesture:\n",
    "                current_gesture = detected_gesture\n",
    "                gesture_start_time = current_time\n",
    "                gesture_confirmed = False\n",
    "            elif (current_time - gesture_start_time) >= gesture_hold_time and not gesture_confirmed:\n",
    "                gesture_confirmed = True\n",
    "                \n",
    "                if (current_time - last_gesture_time) > gesture_cooldown:\n",
    "                    if detected_gesture == \"thumbs_up\" and not timer_running:\n",
    "                        timer_running = True\n",
    "                        start_time = current_time\n",
    "                        last_gesture_time = current_time\n",
    "                        gesture_history.clear()  # Clear history after successful gesture\n",
    "                        print(\"Timer RESUMED (👍) - Squats will now be counted!\")\n",
    "                    elif detected_gesture == \"peace_sign\" and timer_running:\n",
    "                        timer_running = False\n",
    "                        total_elapsed += (current_time - start_time)\n",
    "                        last_gesture_time = current_time\n",
    "                        gesture_history.clear()  # Clear history after successful gesture\n",
    "                        print(f\"Timer PAUSED (✌️) at: {format_time(total_elapsed)} - Squats will NOT be counted while paused\")\n",
    "        else:\n",
    "            current_gesture = None\n",
    "            gesture_confirmed = False\n",
    "        \n",
    "        # Calculate current elapsed time\n",
    "        if timer_running:\n",
    "            elapsed_time = total_elapsed + (current_time - start_time)\n",
    "        else:\n",
    "            elapsed_time = total_elapsed\n",
    "        \n",
    "        # ===== ENHANCED DISPLAY UI =====\n",
    "        # Create main status box with better visibility\n",
    "        cv2.rectangle(image, (0, 0), (650, 250), (40, 40, 40), -1)\n",
    "        cv2.rectangle(image, (0, 0), (650, 250), (255, 255, 255), 2)\n",
    "        \n",
    "        # Timer section\n",
    "        cv2.putText(image, 'TIMER', (20, 35), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "        timer_text = format_time(elapsed_time)\n",
    "        cv2.putText(image, timer_text, (20, 70), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 255, 255), 3)\n",
    "        \n",
    "        # Timer status\n",
    "        status = \"RUNNING\" if timer_running else \"PAUSED\"\n",
    "        status_color = (0, 255, 0) if timer_running else (0, 0, 255)\n",
    "        cv2.putText(image, status, (20, 95), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)\n",
    "        \n",
    "        # Squat counter section\n",
    "        cv2.putText(image, 'SQUATS', (250, 35), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "        \n",
    "        # Display squat counter with different color based on timer status\n",
    "        squat_color = (255, 255, 255) if timer_running else (128, 128, 128)\n",
    "        cv2.putText(image, str(squat_counter), (250, 70), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.4, squat_color, 3)\n",
    "        \n",
    "        # Squat stage with timer status indication\n",
    "        if squat_stage:\n",
    "            stage_color = (255, 255, 255) if timer_running else (128, 128, 128)\n",
    "            cv2.putText(image, squat_stage.upper(), (250, 95), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, stage_color, 2)\n",
    "        \n",
    "        # Squat counting status indicator\n",
    "        counting_status = \"COUNTING\" if timer_running else \"NOT COUNTING\"\n",
    "        counting_color = (0, 255, 0) if timer_running else (255, 0, 0)\n",
    "        cv2.putText(image, counting_status, (250, 115), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, counting_color, 2)\n",
    "        \n",
    "        # Average angle display\n",
    "        try:\n",
    "            angle_color = (255, 255, 255) if timer_running else (128, 128, 128)\n",
    "            cv2.putText(image, f'AVG: {int(avg_angle)}°', (450, 70), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, angle_color, 2)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Gesture detection status\n",
    "        gesture_status = \"None\"\n",
    "        gesture_color = (128, 128, 128)\n",
    "        if detected_gesture == \"thumbs_up\":\n",
    "            gesture_status = \"👍 THUMBS UP\"\n",
    "            gesture_color = (0, 255, 0)\n",
    "        elif detected_gesture == \"peace_sign\":\n",
    "            gesture_status = \"✌️ PEACE SIGN\"\n",
    "            gesture_color = (0, 0, 255)\n",
    "        \n",
    "        cv2.putText(image, 'GESTURE:', (20, 135), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        cv2.putText(image, gesture_status, (20, 155), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, gesture_color, 2)\n",
    "        \n",
    "        # Gesture hold progress\n",
    "        if current_gesture and not gesture_confirmed:\n",
    "            progress = min((current_time - gesture_start_time) / gesture_hold_time, 1.0)\n",
    "            cv2.putText(image, f'Hold: {progress:.0%}', (20, 175), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "        \n",
    "        # Cooldown indicator\n",
    "        if (current_time - last_gesture_time) < gesture_cooldown:\n",
    "            cooldown_remaining = gesture_cooldown - (current_time - last_gesture_time)\n",
    "            cv2.putText(image, f'Cooldown: {cooldown_remaining:.1f}s', \n",
    "                        (20, 195),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 100, 0), 1)\n",
    "        \n",
    "        # Timer-controlled squat counting notice\n",
    "        notice_color = (0, 255, 0) if timer_running else (255, 100, 0)\n",
    "        notice_text = \"✓ Squats counting active\" if timer_running else \"⚠ Start timer to count squats\"\n",
    "        cv2.putText(image, notice_text, (20, 220), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, notice_color, 2)\n",
    "        \n",
    "        # Enhanced instructions\n",
    "        instructions = [\n",
    "            \"👍 Thumbs up: Resume timer & start counting squats\",\n",
    "            \"✌️ Peace: Pause timer & stop counting squats\",\n",
    "            \"🏋️ Squats only counted when timer is RUNNING\",\n",
    "            \"Press 'R' to reset all, 'Q' to quit\"\n",
    "        ]\n",
    "        \n",
    "        y_offset = frame_height - 85\n",
    "        for instruction in instructions:\n",
    "            cv2.putText(image, instruction, (10, y_offset), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "            y_offset += 18\n",
    "        \n",
    "        # Render pose landmarks\n",
    "        if pose_results.pose_landmarks:\n",
    "            # Change pose landmark colors based on timer status\n",
    "            pose_color = (245, 117, 66) if timer_running else (100, 100, 100)\n",
    "            connection_color = (245, 66, 230) if timer_running else (120, 120, 120)\n",
    "            \n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=pose_color, thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=connection_color, thickness=2, circle_radius=2)\n",
    "            )\n",
    "        \n",
    "        # Show frame\n",
    "        cv2.imshow('Enhanced Fitness Tracker - Timer-Controlled Squat Counter', image)\n",
    "        \n",
    "        # Handle keyboard input\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('r') or key == ord('R'):\n",
    "            # Reset everything\n",
    "            timer_running = False\n",
    "            start_time = 0\n",
    "            elapsed_time = 0\n",
    "            total_elapsed = 0\n",
    "            squat_counter = 0\n",
    "            squat_stage = None\n",
    "            gesture_history.clear()\n",
    "            print(\"Timer and squat counter RESET\")\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7a00e8-9302-4bdf-b1ce-5f75280d9ba4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 657\u001b[0m\n\u001b[0;32m    654\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 657\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[2], line 639\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    636\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGesture-Controlled Jumping Jack Counter\u001b[39m\u001b[38;5;124m'\u001b[39m, image)\n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# Handle keyboard input\u001b[39;00m\n\u001b[1;32m--> 639\u001b[0m key \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "class GestureTimer:\n",
    "    \"\"\"\n",
    "    Robust gesture recognition system for timer control.\n",
    "    Supports thumbs up (resume) and peace sign (pause) gestures.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, confidence_threshold=0.8, smoothing_frames=5):\n",
    "        \"\"\"\n",
    "        Initialize the gesture timer.\n",
    "        \n",
    "        Args:\n",
    "            confidence_threshold: Minimum confidence for gesture detection\n",
    "            smoothing_frames: Number of frames for gesture smoothing\n",
    "        \"\"\"\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.smoothing_frames = smoothing_frames\n",
    "        \n",
    "        # Timer state\n",
    "        self.is_running = False\n",
    "        self.start_time = None\n",
    "        self.elapsed_time = 0.0\n",
    "        self.pause_time = None\n",
    "        \n",
    "        # Gesture detection smoothing\n",
    "        self.gesture_history = deque(maxlen=smoothing_frames)\n",
    "        self.last_gesture = None\n",
    "        self.gesture_change_time = time.time()\n",
    "        \n",
    "        # Initialize MediaPipe hands\n",
    "        self.hands = mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "    \n",
    "    def get_hand_landmarks(self, image):\n",
    "        \"\"\"Extract hand landmarks from image.\"\"\"\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = self.hands.process(rgb_image)\n",
    "        return results.multi_hand_landmarks, results.multi_handedness\n",
    "    \n",
    "    def calculate_finger_angles(self, landmarks):\n",
    "        \"\"\"\n",
    "        Calculate angles for each finger to determine hand pose.\n",
    "        Returns angles for thumb, index, middle, ring, pinky.\n",
    "        \"\"\"\n",
    "        def get_angle(p1, p2, p3):\n",
    "            \"\"\"Calculate angle between three points.\"\"\"\n",
    "            v1 = np.array([p1.x - p2.x, p1.y - p2.y])\n",
    "            v2 = np.array([p3.x - p2.x, p3.y - p2.y])\n",
    "            \n",
    "            cosine = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "            cosine = np.clip(cosine, -1.0, 1.0)\n",
    "            angle = np.arccos(cosine)\n",
    "            return np.degrees(angle)\n",
    "        \n",
    "        # Landmark indices for finger joints\n",
    "        finger_joints = [\n",
    "            [1, 2, 3],    # Thumb\n",
    "            [5, 6, 7],    # Index\n",
    "            [9, 10, 11],  # Middle\n",
    "            [13, 14, 15], # Ring\n",
    "            [17, 18, 19]  # Pinky\n",
    "        ]\n",
    "        \n",
    "        angles = []\n",
    "        for joints in finger_joints:\n",
    "            try:\n",
    "                angle = get_angle(\n",
    "                    landmarks[joints[0]], \n",
    "                    landmarks[joints[1]], \n",
    "                    landmarks[joints[2]]\n",
    "                )\n",
    "                angles.append(angle)\n",
    "            except:\n",
    "                angles.append(0)\n",
    "        \n",
    "        return angles\n",
    "    \n",
    "    def detect_thumbs_up(self, landmarks, handedness):\n",
    "        \"\"\"\n",
    "        Detect thumbs up gesture using multiple geometric features.\n",
    "        Works from multiple angles by analyzing finger positions and orientations.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get key landmarks\n",
    "            thumb_tip = landmarks[4]\n",
    "            thumb_mcp = landmarks[2]\n",
    "            index_tip = landmarks[8]\n",
    "            index_mcp = landmarks[5]\n",
    "            middle_tip = landmarks[12]\n",
    "            wrist = landmarks[0]\n",
    "            \n",
    "            # Calculate relative positions\n",
    "            thumb_height = abs(thumb_tip.y - wrist.y)\n",
    "            index_height = abs(index_tip.y - wrist.y)\n",
    "            middle_height = abs(middle_tip.y - wrist.y)\n",
    "            \n",
    "            # Check if thumb is extended upward relative to other fingers\n",
    "            thumb_extended = thumb_height > max(index_height, middle_height) * 0.8\n",
    "            \n",
    "            # Check finger curl states using angles\n",
    "            angles = self.calculate_finger_angles(landmarks)\n",
    "            thumb_angle, index_angle, middle_angle, ring_angle, pinky_angle = angles\n",
    "            \n",
    "            # Fingers should be curled (higher angles indicate more curl)\n",
    "            fingers_curled = (\n",
    "                index_angle > 90 and \n",
    "                middle_angle > 90 and \n",
    "                ring_angle > 90 and \n",
    "                pinky_angle > 90\n",
    "            )\n",
    "            \n",
    "            # Additional check: thumb should be pointing away from palm\n",
    "            thumb_distance_from_palm = np.sqrt(\n",
    "                (thumb_tip.x - index_mcp.x)**2 + \n",
    "                (thumb_tip.y - index_mcp.y)**2\n",
    "            )\n",
    "            \n",
    "            # Combine all conditions\n",
    "            is_thumbs_up = (\n",
    "                thumb_extended and \n",
    "                fingers_curled and \n",
    "                thumb_distance_from_palm > 0.1\n",
    "            )\n",
    "            \n",
    "            return is_thumbs_up, 0.9 if is_thumbs_up else 0.1\n",
    "            \n",
    "        except Exception as e:\n",
    "            return False, 0.0\n",
    "    \n",
    "    def detect_peace_sign(self, landmarks, handedness):\n",
    "        \"\"\"\n",
    "        Detect peace sign (V sign) gesture.\n",
    "        Analyzes index and middle finger extension while other fingers are curled.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get key landmarks\n",
    "            index_tip = landmarks[8]\n",
    "            index_pip = landmarks[6]\n",
    "            middle_tip = landmarks[12]\n",
    "            middle_pip = landmarks[10]\n",
    "            ring_tip = landmarks[16]\n",
    "            pinky_tip = landmarks[20]\n",
    "            wrist = landmarks[0]\n",
    "            \n",
    "            # Calculate finger extensions\n",
    "            index_extended = abs(index_tip.y - wrist.y) > abs(index_pip.y - wrist.y)\n",
    "            middle_extended = abs(middle_tip.y - wrist.y) > abs(middle_pip.y - wrist.y)\n",
    "            \n",
    "            # Check if index and middle fingers are spread apart\n",
    "            finger_spread = np.sqrt(\n",
    "                (index_tip.x - middle_tip.x)**2 + \n",
    "                (index_tip.y - middle_tip.y)**2\n",
    "            )\n",
    "            \n",
    "            # Calculate finger angles\n",
    "            angles = self.calculate_finger_angles(landmarks)\n",
    "            thumb_angle, index_angle, middle_angle, ring_angle, pinky_angle = angles\n",
    "            \n",
    "            # Index and middle should be extended (lower angles)\n",
    "            # Ring and pinky should be curled (higher angles)\n",
    "            fingers_correct = (\n",
    "                index_angle < 60 and \n",
    "                middle_angle < 60 and \n",
    "                ring_angle > 90 and \n",
    "                pinky_angle > 90\n",
    "            )\n",
    "            \n",
    "            # Check vertical separation between extended fingers\n",
    "            vertical_separation = abs(index_tip.y - middle_tip.y) < 0.05\n",
    "            \n",
    "            is_peace = (\n",
    "                index_extended and \n",
    "                middle_extended and \n",
    "                fingers_correct and \n",
    "                finger_spread > 0.05 and\n",
    "                vertical_separation\n",
    "            )\n",
    "            \n",
    "            return is_peace, 0.9 if is_peace else 0.1\n",
    "            \n",
    "        except Exception as e:\n",
    "            return False, 0.0\n",
    "    \n",
    "    def recognize_gesture(self, image):\n",
    "        \"\"\"\n",
    "        Main gesture recognition function.\n",
    "        Returns gesture type and confidence score.\n",
    "        \"\"\"\n",
    "        hand_landmarks, handedness = self.get_hand_landmarks(image)\n",
    "        \n",
    "        if not hand_landmarks:\n",
    "            return None, 0.0\n",
    "        \n",
    "        best_gesture = None\n",
    "        best_confidence = 0.0\n",
    "        \n",
    "        # Check all detected hands\n",
    "        for i, landmarks in enumerate(hand_landmarks):\n",
    "            hand_type = handedness[i].classification[0].label if handedness else \"Unknown\"\n",
    "            \n",
    "            # Check for thumbs up\n",
    "            thumbs_up, thumbs_confidence = self.detect_thumbs_up(landmarks.landmark, hand_type)\n",
    "            if thumbs_up and thumbs_confidence > best_confidence:\n",
    "                best_gesture = \"thumbs_up\"\n",
    "                best_confidence = thumbs_confidence\n",
    "            \n",
    "            # Check for peace sign\n",
    "            peace, peace_confidence = self.detect_peace_sign(landmarks.landmark, hand_type)\n",
    "            if peace and peace_confidence > best_confidence:\n",
    "                best_gesture = \"peace\"\n",
    "                best_confidence = peace_confidence\n",
    "        \n",
    "        return best_gesture, best_confidence\n",
    "    \n",
    "    def update_gesture_state(self, image):\n",
    "        \"\"\"\n",
    "        Update timer state based on recognized gestures.\n",
    "        Includes smoothing to prevent false positives.\n",
    "        \"\"\"\n",
    "        gesture, confidence = self.recognize_gesture(image)\n",
    "        \n",
    "        # Add to gesture history for smoothing\n",
    "        if confidence > self.confidence_threshold:\n",
    "            self.gesture_history.append(gesture)\n",
    "        else:\n",
    "            self.gesture_history.append(None)\n",
    "        \n",
    "        # Determine smoothed gesture\n",
    "        if len(self.gesture_history) == self.smoothing_frames:\n",
    "            gesture_counts = {}\n",
    "            for g in self.gesture_history:\n",
    "                if g:\n",
    "                    gesture_counts[g] = gesture_counts.get(g, 0) + 1\n",
    "            \n",
    "            # Require majority consensus\n",
    "            required_count = self.smoothing_frames // 2 + 1\n",
    "            smoothed_gesture = None\n",
    "            \n",
    "            for g, count in gesture_counts.items():\n",
    "                if count >= required_count:\n",
    "                    smoothed_gesture = g\n",
    "                    break\n",
    "            \n",
    "            # Update timer state based on smoothed gesture\n",
    "            current_time = time.time()\n",
    "            \n",
    "            if smoothed_gesture != self.last_gesture:\n",
    "                if smoothed_gesture == \"thumbs_up\" and not self.is_running:\n",
    "                    self.resume_timer()\n",
    "                elif smoothed_gesture == \"peace\" and self.is_running:\n",
    "                    self.pause_timer()\n",
    "                \n",
    "                self.last_gesture = smoothed_gesture\n",
    "                self.gesture_change_time = current_time\n",
    "        \n",
    "        return gesture, confidence\n",
    "    \n",
    "    def resume_timer(self):\n",
    "        \"\"\"Resume the timer.\"\"\"\n",
    "        if not self.is_running:\n",
    "            self.is_running = True\n",
    "            if self.pause_time:\n",
    "                # Resume from where we paused\n",
    "                pause_duration = time.time() - self.pause_time\n",
    "                self.start_time += pause_duration\n",
    "                self.pause_time = None\n",
    "            else:\n",
    "                # Start fresh\n",
    "                self.start_time = time.time()\n",
    "    \n",
    "    def pause_timer(self):\n",
    "        \"\"\"Pause the timer.\"\"\"\n",
    "        if self.is_running:\n",
    "            self.is_running = False\n",
    "            self.pause_time = time.time()\n",
    "            if self.start_time:\n",
    "                self.elapsed_time = self.pause_time - self.start_time\n",
    "    \n",
    "    def get_elapsed_time(self):\n",
    "        \"\"\"Get current elapsed time.\"\"\"\n",
    "        if self.is_running and self.start_time:\n",
    "            return time.time() - self.start_time\n",
    "        elif self.pause_time and self.start_time:\n",
    "            return self.pause_time - self.start_time\n",
    "        return 0.0\n",
    "    \n",
    "    def reset_timer(self):\n",
    "        \"\"\"Reset the timer to initial state.\"\"\"\n",
    "        self.is_running = False\n",
    "        self.start_time = None\n",
    "        self.elapsed_time = 0.0\n",
    "        self.pause_time = None\n",
    "        self.gesture_history.clear()\n",
    "        self.last_gesture = None\n",
    "\n",
    "# Function to calculate angle between three points\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calculate angle between three points\n",
    "    a, b, c are lists/arrays of [x, y] coordinates\n",
    "    b is the vertex point\n",
    "    \"\"\"\n",
    "    a = np.array(a)  # First point\n",
    "    b = np.array(b)  # Vertex point\n",
    "    c = np.array(c)  # End point\n",
    "    \n",
    "    # Calculate vectors\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    \n",
    "    # Calculate angle using dot product\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    \n",
    "    # Convert to degrees\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Function to calculate distance between two points\n",
    "def calculate_distance(point1, point2):\n",
    "    \"\"\"Calculate Euclidean distance between two points\"\"\"\n",
    "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "# Function to detect jumping jack stage\n",
    "def detect_jumping_jack_stage(left_wrist, right_wrist, left_ankle, right_ankle, \n",
    "                            left_shoulder, right_shoulder, left_hip, right_hip):\n",
    "    \"\"\"\n",
    "    Detect if person is in 'up' or 'down' stage of jumping jack\n",
    "    Up stage: arms raised, legs spread apart\n",
    "    Down stage: arms down, legs together\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate arm spread (distance between wrists relative to shoulders)\n",
    "    wrist_distance = calculate_distance(left_wrist, right_wrist)\n",
    "    shoulder_distance = calculate_distance(left_shoulder, right_shoulder)\n",
    "    arm_spread_ratio = wrist_distance / shoulder_distance if shoulder_distance > 0 else 0\n",
    "    \n",
    "    # Calculate leg spread (distance between ankles relative to hips)\n",
    "    ankle_distance = calculate_distance(left_ankle, right_ankle)\n",
    "    hip_distance = calculate_distance(left_hip, right_hip)\n",
    "    leg_spread_ratio = ankle_distance / hip_distance if hip_distance > 0 else 0\n",
    "    \n",
    "    # Calculate arm height (average wrist height relative to shoulders)\n",
    "    avg_wrist_y = (left_wrist[1] + right_wrist[1]) / 2\n",
    "    avg_shoulder_y = (left_shoulder[1] + right_shoulder[1]) / 2\n",
    "    arm_height_ratio = (avg_shoulder_y - avg_wrist_y)  # Positive when arms are raised\n",
    "    \n",
    "    # Thresholds for detection (adjust based on testing)\n",
    "    ARM_SPREAD_UP_THRESHOLD = 1.8    # Arms spread wide\n",
    "    ARM_SPREAD_DOWN_THRESHOLD = 1.2  # Arms close to body\n",
    "    LEG_SPREAD_UP_THRESHOLD = 1.8    # Legs spread apart\n",
    "    LEG_SPREAD_DOWN_THRESHOLD = 1.2  # Legs close together\n",
    "    ARM_HEIGHT_THRESHOLD = 0.1       # Arms raised above shoulders\n",
    "    \n",
    "    # Determine stage based on arm and leg positions\n",
    "    arms_up = (arm_spread_ratio > ARM_SPREAD_UP_THRESHOLD and arm_height_ratio > ARM_HEIGHT_THRESHOLD)\n",
    "    arms_down = (arm_spread_ratio < ARM_SPREAD_DOWN_THRESHOLD and arm_height_ratio < -ARM_HEIGHT_THRESHOLD)\n",
    "    \n",
    "    legs_apart = leg_spread_ratio > LEG_SPREAD_UP_THRESHOLD\n",
    "    legs_together = leg_spread_ratio < LEG_SPREAD_DOWN_THRESHOLD\n",
    "    \n",
    "    if arms_up and legs_apart:\n",
    "        return 'up', arm_spread_ratio, leg_spread_ratio, arm_height_ratio\n",
    "    elif arms_down and legs_together:\n",
    "        return 'down', arm_spread_ratio, leg_spread_ratio, arm_height_ratio\n",
    "    else:\n",
    "        return 'transition', arm_spread_ratio, leg_spread_ratio, arm_height_ratio\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format seconds into MM:SS format.\"\"\"\n",
    "    minutes = int(seconds // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    return f\"{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "# Main application\n",
    "def main():\n",
    "    # Initialize video capture\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Initialize gesture timer\n",
    "    gesture_timer = GestureTimer(confidence_threshold=0.7, smoothing_frames=3)\n",
    "    \n",
    "    # Jumping jack counter variables\n",
    "    jj_counter = 0\n",
    "    jj_stage = None\n",
    "    \n",
    "    # Set up Mediapipe pose instance\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame\")\n",
    "                break\n",
    "                \n",
    "            # Get frame dimensions\n",
    "            frame_height, frame_width = frame.shape[:2]\n",
    "            \n",
    "            # Update gesture timer state\n",
    "            current_gesture, gesture_confidence = gesture_timer.update_gesture_state(frame)\n",
    "            \n",
    "            # Recolor image to RGB for pose detection\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            \n",
    "            # Make pose detection\n",
    "            results = pose.process(image)\n",
    "            \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Initialize variables\n",
    "            left_knee_angle = 0\n",
    "            right_knee_angle = 0\n",
    "            current_stage = 'none'\n",
    "            arm_spread_ratio = 0\n",
    "            leg_spread_ratio = 0\n",
    "            arm_height_ratio = 0\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                if results.pose_landmarks:\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                    \n",
    "                    # Get coordinates for required landmarks (normalized)\n",
    "                    left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                                   landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "                    right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                                    landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "                    \n",
    "                    left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "                                landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "                    right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                                 landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "                    \n",
    "                    left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n",
    "                                landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "                    right_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                                 landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "                    \n",
    "                    left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                               landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "                    right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                                landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "                    \n",
    "                    left_knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x,\n",
    "                                landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "                    right_knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\n",
    "                                 landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "                    \n",
    "                    left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x,\n",
    "                                 landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "                    right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                                  landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "                    \n",
    "                    # Calculate knee angles for both legs (hip-knee-ankle)\n",
    "                    left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "                    right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "                    \n",
    "                    # Detect jumping jack stage\n",
    "                    current_stage, arm_spread_ratio, leg_spread_ratio, arm_height_ratio = detect_jumping_jack_stage(\n",
    "                        left_wrist, right_wrist, left_ankle, right_ankle,\n",
    "                        left_shoulder, right_shoulder, left_hip, right_hip\n",
    "                    )\n",
    "                    \n",
    "                    # Jumping jack counting logic - ONLY COUNT WHEN TIMER IS RUNNING\n",
    "                    if gesture_timer.is_running:\n",
    "                        if current_stage == 'up':\n",
    "                            jj_stage = 'up'\n",
    "                        elif current_stage == 'down' and jj_stage == 'up':\n",
    "                            jj_stage = 'down'\n",
    "                            jj_counter += 1\n",
    "                            print(f\"Jumping jack completed! Count: {jj_counter}\")\n",
    "                    \n",
    "                    # Convert coordinates to pixel coordinates for visualization\n",
    "                    left_wrist_coords = tuple(np.multiply(left_wrist, [frame_width, frame_height]).astype(int))\n",
    "                    right_wrist_coords = tuple(np.multiply(right_wrist, [frame_width, frame_height]).astype(int))\n",
    "                    left_ankle_coords = tuple(np.multiply(left_ankle, [frame_width, frame_height]).astype(int))\n",
    "                    right_ankle_coords = tuple(np.multiply(right_ankle, [frame_width, frame_height]).astype(int))\n",
    "                    left_knee_coords = tuple(np.multiply(left_knee, [frame_width, frame_height]).astype(int))\n",
    "                    right_knee_coords = tuple(np.multiply(right_knee, [frame_width, frame_height]).astype(int))\n",
    "                    \n",
    "                    # Draw indicators based on current stage\n",
    "                    if current_stage == 'up':\n",
    "                        # Highlight key points in green when in up position\n",
    "                        cv2.circle(image, left_wrist_coords, 8, (0, 255, 0), -1)\n",
    "                        cv2.circle(image, right_wrist_coords, 8, (0, 255, 0), -1)\n",
    "                        cv2.circle(image, left_ankle_coords, 8, (0, 255, 0), -1)\n",
    "                        cv2.circle(image, right_ankle_coords, 8, (0, 255, 0), -1)\n",
    "                    elif current_stage == 'down':\n",
    "                        # Highlight key points in blue when in down position\n",
    "                        cv2.circle(image, left_wrist_coords, 8, (255, 0, 0), -1)\n",
    "                        cv2.circle(image, right_wrist_coords, 8, (255, 0, 0), -1)\n",
    "                        cv2.circle(image, left_ankle_coords, 8, (255, 0, 0), -1)\n",
    "                        cv2.circle(image, right_ankle_coords, 8, (255, 0, 0), -1)\n",
    "                    else:\n",
    "                        # Neutral color during transition\n",
    "                        cv2.circle(image, left_wrist_coords, 6, (0, 255, 255), -1)\n",
    "                        cv2.circle(image, right_wrist_coords, 6, (0, 255, 255), -1)\n",
    "                        cv2.circle(image, left_ankle_coords, 6, (0, 255, 255), -1)\n",
    "                        cv2.circle(image, right_ankle_coords, 6, (0, 255, 255), -1)\n",
    "                    \n",
    "                    # Draw knee angle indicators\n",
    "                    cv2.circle(image, left_knee_coords, 6, (255, 255, 0), -1)\n",
    "                    cv2.circle(image, right_knee_coords, 6, (255, 255, 0), -1)\n",
    "                    \n",
    "                    # Display knee angles\n",
    "                    cv2.putText(image, f'{int(left_knee_angle)}°',\n",
    "                               (left_knee_coords[0] - 30, left_knee_coords[1] - 15),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "                    \n",
    "                    cv2.putText(image, f'{int(right_knee_angle)}°',\n",
    "                               (right_knee_coords[0] + 10, right_knee_coords[1] - 15),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(\"Error:\", e)\n",
    "            \n",
    "            # Setup main status display (expanded for timer)\n",
    "            cv2.rectangle(image, (10, 10), (600, 280), (50, 50, 50), -1)\n",
    "            cv2.rectangle(image, (10, 10), (600, 280), (255, 255, 255), 2)\n",
    "            \n",
    "            # Display jumping jack count\n",
    "            cv2.putText(image, 'JUMPING JACKS', (20, 40), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(jj_counter), (20, 80), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "            \n",
    "            # Display current stage\n",
    "            stage_color = (0, 255, 0) if current_stage == 'up' else (255, 0, 0) if current_stage == 'down' else (0, 255, 255)\n",
    "            cv2.putText(image, 'STAGE', (200, 40), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, current_stage.upper(), (200, 75), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, stage_color, 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Display timer information\n",
    "            timer_color = (0, 255, 0) if gesture_timer.is_running else (255, 255, 0)\n",
    "            timer_status = \"RUNNING\" if gesture_timer.is_running else \"PAUSED\"\n",
    "            elapsed_time = gesture_timer.get_elapsed_time()\n",
    "            \n",
    "            cv2.putText(image, 'TIMER', (350, 40), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, timer_status, (350, 65), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, timer_color, 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, format_time(elapsed_time), (350, 90), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Display gesture information\n",
    "            cv2.putText(image, 'GESTURE CONTROL', (20, 120), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            gesture_text = \"None\"\n",
    "            gesture_color = (128, 128, 128)\n",
    "            \n",
    "            if current_gesture == \"thumbs_up\":\n",
    "                gesture_text = \"👍 THUMBS UP\"\n",
    "                gesture_color = (0, 255, 0)\n",
    "            elif current_gesture == \"peace\":\n",
    "                gesture_text = \"✌️ PEACE SIGN\"\n",
    "                gesture_color = (255, 255, 0)\n",
    "            \n",
    "            cv2.putText(image, gesture_text, (20, 145), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, gesture_color, 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, f'Confidence: {gesture_confidence:.2f}', (20, 170), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # Display knee angles\n",
    "            cv2.putText(image, 'KNEE ANGLES', (350, 120), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, f'L: {int(left_knee_angle)}°', (350, 145), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, f'R: {int(right_knee_angle)}°', (350, 165), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Display detection metrics\n",
    "            cv2.putText(image, 'DETECTION METRICS', (20, 200), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, f'Arm Spread: {arm_spread_ratio:.2f}', (20, 220), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, f'Leg Spread: {leg_spread_ratio:.2f}', (20, 240), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, f'Arm Height: {arm_height_ratio:.2f}', (20, 260), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # Display current internal stage\n",
    "            internal_stage = jj_stage if jj_stage else 'None'\n",
    "            cv2.putText(image, f'Internal: {internal_stage}', (350, 200), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # Render pose detections\n",
    "            if results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                    mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "                )\n",
    "            \n",
    "            # Draw hand landmarks for gesture debugging (optional)\n",
    "            hand_landmarks, handedness = gesture_timer.get_hand_landmarks(image)\n",
    "            if hand_landmarks:\n",
    "                for hand_landmark in hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image, hand_landmark, mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                        mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "                    )\n",
    "            \n",
    "            # Add instructions\n",
    "            instruction_y = frame_height - 60\n",
    "            cv2.putText(image, 'CONTROLS: 👍 Thumbs Up = Resume Timer | ✌️ Peace Sign = Pause Timer | Q = Quit', \n",
    "                        (10, instruction_y), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'NOTE: Reps only count when timer is running!', \n",
    "                        (10, instruction_y + 20), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'Hold gesture steady for 2-3 seconds for best recognition', \n",
    "                        (10, instruction_y + 40), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # Display the image\n",
    "            cv2.imshow('Gesture-Controlled Jumping Jack Counter', image)\n",
    "            \n",
    "            # Handle keyboard input\n",
    "            key = cv2.waitKey(10) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('r'):  # Reset timer manually\n",
    "                gesture_timer.reset_timer()\n",
    "                jj_counter = 0\n",
    "                jj_stage = None\n",
    "                print(\"Timer and counter reset!\")\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(f\"\\nFinal Jumping Jack Results:\")\n",
    "    print(f\"Total Jumping Jacks: {jj_counter}\")\n",
    "    print(f\"Total Workout Time: {format_time(gesture_timer.get_elapsed_time())}\")\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab6a8d-d4e7-44d9-858f-45824e73d8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
